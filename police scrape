# Script to dynamically expand and extract content surrounding sections
# Purpose: Extract labeled data from a webpage and handle varying content structures

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import logging
import time
import csv

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Set up the Selenium WebDriver
options = webdriver.ChromeOptions()
options.add_argument("--headless")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

try:
    # Open the URL
    url = "https://www.gov.il/he/departments/dynamiccollectors/hesdermutne?skip=0"
    logging.info(f"Accessing URL: {url}")
    driver.get(url)

    # Wait for the page to load
    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
    logging.info("Page loaded successfully.")

    # Locate and click all expandable sections
    expand_buttons = driver.find_elements(By.XPATH, "//div[@ng-click='item.isDisplay = !item.isDisplay']")
    for button in expand_buttons:
        driver.execute_script("arguments[0].click();", button)
        time.sleep(1)  # Wait for content to expand
    logging.info(f"Clicked {len(expand_buttons)} expand buttons.")

    # Extract page source after expanding all sections
    page_source = driver.page_source

    # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(page_source, "html.parser")

    # Define column titles
    titles = [
        "מספר תיק",
        "שלוחה",
        "תאריך",
        "תאריך עברי",
        "תיאור התיק",
        "תיאור העובדות המהוות עבירה שבהן הודה החשוד",
        "הוראות החיקוק שפורטו בהסדר",
        "תנאי הסדר",
        "נימוקים לסגירת התיק בהסדר"
    ]

    extracted_data = []

    # Find all sections containing the data
    sections = soup.find_all("div", class_="row row-gov")
    logging.info(f"Found {len(sections)} sections to parse.")

    for section in sections:
        row = []
        for title in titles:
            content = ""
            try:
                # Locate data by label or header
                label = section.find("label", string=title)
                if label:
                    content_element = label.find_next("span") or label.find_next("bdi")
                    content = content_element.get_text(strip=True) if content_element else ""
                else:
                    # Handle special cases for headers or other unique formats
                    if title in ["תיאור העובדות המהוות עבירה שבהן הודה החשוד", "תנאי הסדר", "נימוקים לסגירת התיק בהסדר"]:
                        header = section.find("h3", string=title)
                        if header:
                            if header.find_next("li"):
                                content = "\n".join(li.get_text(strip=True) for li in header.find_next("ul").find_all("li"))
                            elif header.find_next("table"):
                                content = "\n".join(
                                    [row.get_text(strip=True) for row in header.find_next("table").find_all("td")]
                                )
                    elif title == "תיאור התיק":
                        content_element = section.find("div", class_="dark-gray-txt")
                        content = content_element.get_text(strip=True) if content_element else ""
                row.append(content)
                logging.info(f"Extracted content for '{title}': {content[:50]}...")  # Log truncated content
            except Exception as e:
                logging.error(f"Error extracting content for '{title}': {e}")
                row.append("")  # Leave blank if data extraction fails

        extracted_data.append(row)

    # Save the extracted data to a CSV file
    output_file = "extracted_content_final.csv"
    with open(output_file, "w", encoding="utf-8", newline="") as file:
        writer = csv.writer(file)
        writer.writerow(titles)
        writer.writerows(extracted_data)
    logging.info(f"Extracted content saved to {output_file}")

except Exception as e:
    logging.error(f"An error occurred: {e}")

finally:
    driver.quit()
    logging.info("Driver closed.")
