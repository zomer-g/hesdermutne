# Import necessary libraries
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import time
import csv

# Set up Chrome options for headless mode (no browser UI)
options = Options()
options.add_argument("--headless")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

# Initialize the WebDriver
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)


def extract_relevant_blocks(url):
    """Extract relevant content blocks and specific details from a given URL."""
    driver.get(url)  # Open the URL

    # Wait for the page to load completely
    time.sleep(5)

    # Click all "view more" buttons to expand the content
    expand_buttons = driver.find_elements(By.XPATH, "//div[@ng-click='item.isDisplay = !item.isDisplay']")
    for button in expand_buttons:
        driver.execute_script("arguments[0].click();", button)
        time.sleep(1)  # Wait for content to load

    # Find the relevant blocks by the div structure
    blocks = driver.find_elements(By.XPATH, "//div[@class='row row-gov']")
    extracted_data = []

    # Use a set to track processed blocks by their unique identifiers (e.g., case number)
    processed_cases = set()

    for block in blocks:
        try:
            case_number = block.find_element(By.XPATH, ".//label[contains(text(), 'מספר תיק')]/following-sibling::span").text
            if case_number in processed_cases:
                continue  # Skip duplicate blocks

            processed_cases.add(case_number)  # Mark this case as processed

            branch = block.find_element(By.XPATH, ".//label[contains(text(), 'שלוחה')]/following-sibling::span").text
            date = block.find_element(By.XPATH, ".//label[contains(text(), 'תאריך')]/following-sibling::span").text
            hebrew_date = block.find_element(By.XPATH, ".//label[contains(text(), 'תאריך עברי')]/following-sibling::span").text
            description = block.find_element(By.XPATH, ".//h3[contains(text(), 'תיאור העובדות')]/following-sibling::li").text
            legislation = block.find_element(By.XPATH, ".//h3[contains(text(), 'הוראות החיקוק')]/following-sibling::li").text
            conditions = block.find_element(By.XPATH, ".//h3[contains(text(), 'תנאי הסדר')]/following-sibling::li").text
            reasoning = block.find_element(By.XPATH, ".//h3[contains(text(), 'נימוקים לסגירת התיק')]/following-sibling::div").text

            extracted_data.append({
                "מספר תיק": case_number,
                "שלוחה": branch,
                "תאריך": date,
                "תאריך עברי": hebrew_date,
                "תיאור העובדות המהוות עבירה שבהן הודה החשוד": description,
                "הוראות החיקוק שפורטו בהסדר": legislation,
                "תנאי הסדר": conditions,
                "נימוקים לסגירת התיק בהסדר": reasoning,
            })
        except Exception as e:
            print(f"Error extracting block: {e}")

    return extracted_data


def save_to_csv(data, filename):
    """Save the extracted data to a CSV file."""
    with open(filename, "w", encoding="utf-8", newline="") as file:
        writer = csv.DictWriter(file, fieldnames=[
            "מספר תיק", "שלוחה", "תאריך", "תאריך עברי",
            "תיאור העובדות המהוות עבירה שבהן הודה החשוד",
            "הוראות החיקוק שפורטו בהסדר",
            "תנאי הסדר",
            "נימוקים לסגירת התיק בהסדר",
        ])
        writer.writeheader()
        writer.writerows(data)
    print(f"Content blocks have been saved to '{filename}'.")


def process_all_pages(base_url, filename):
    """Process pages iteratively until no content is found."""
    page_number = 0
    all_data = []

    while True:
        url = f"{base_url}?skip={page_number * 10}"
        print(f"Processing URL: {url}")
        data = extract_relevant_blocks(url)

        if not data:
            print("No content found on this page. Stopping.")
            break

        all_data.extend(data)
        page_number += 1

    # Save the aggregated data to a CSV file
    save_to_csv(all_data, filename)


try:
    # Base URL to process
    base_url = "https://www.gov.il/he/departments/dynamiccollectors/hesdermutne"
    output_filename = "updated_blocks.csv"

    # Process all pages dynamically
    process_all_pages(base_url, output_filename)

except Exception as e:
    print(f"An error occurred: {e}")

finally:
    driver.quit()
    print("Driver closed.")
